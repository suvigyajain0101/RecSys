{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retrieval.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBXvsF6Hn92N4fES4SdT28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recommending movies: retrieval\n",
        "\n",
        "Real world Recommender systems are often made up of following steps - \n",
        "\n",
        "* `Retrieval` :   Selecting few thousands of possible candidates to recommend from a set of millions. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
        "\n",
        "* `Ranking` : Drill down thousand possible candidates to few hundreds\n",
        "\n",
        "* `Post-Ranking` : Further refine the candidates to few dozens - Might be helpful in case user is logged from mobile device where screen real estate is limited\n",
        "\n",
        "In this notebook, we will go over Retrieval stage of the Recommender System. Retrieval models are often composed of two sub-models:\n",
        "\n",
        "* A `query model` computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
        "* A `candidate model` computing the candidate representation (an equally-sized vector) using the candidate features\n",
        "\n",
        "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query."
      ],
      "metadata": {
        "id": "V4k3hGcra5I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The Movielens dataset is a classic dataset from the GroupLens research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.\n",
        "\n",
        "The data can be treated in two ways:\n",
        "\n",
        "1. It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of `implicit feedback`, where users' watches tell us which things they prefer to see and which they'd rather not see.\n",
        "\n",
        "2. It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of `explicit feedback`: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.\n",
        "\n",
        "\n",
        "In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example."
      ],
      "metadata": {
        "id": "nO5xXeCdccWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rW2pUo44dDyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-recommenders\n",
        "! pip install -q --upgrade tensorflow-datasets\n",
        "! pip install -q scann\n",
        "! pip install tfds-nightly"
      ],
      "metadata": {
        "id": "b1CkxznJc9gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "1bsMj3gqcxwu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "lkn8USR0dHKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
      ],
      "metadata": {
        "id": "Kkq2B-zqdmh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at features from ratings datasets\n",
        "for x in ratings.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "id": "Nx7NtrFadprt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And features from movies datasets\n",
        "for x in movies.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "id": "eybWAid3ghdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract relevant features for our task - User ID and Movie Title\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "metadata": {
        "id": "WaHwM2qGgndY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split\n",
        "\n",
        "We'll go for 80-20 split"
      ],
      "metadata": {
        "id": "uSiE4aADhRla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "metadata": {
        "id": "SlEBNjjOhffY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also figure out unique user ids and movie titles present in the data.\n",
        "\n",
        "# This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. \n",
        "# To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: \n",
        "# this allows us to look up the corresponding embeddings in our embedding tables."
      ],
      "metadata": {
        "id": "J4zrzoXYhkAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "unique_movie_titles[:10]"
      ],
      "metadata": {
        "id": "ULN-OWCRh73P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementation\n",
        "\n",
        "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
      ],
      "metadata": {
        "id": "8q9OLgV-UHZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The query tower"
      ],
      "metadata": {
        "id": "NCnojYPiUKAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first step is to decide on the dimensionality of the query and candidate representations:\n",
        "embedding_dimension = 32\n",
        "\n",
        "# Higher values will correspond to models that may be more accurate, \n",
        "# but will also be slower to fit and more prone to overfitting."
      ],
      "metadata": {
        "id": "NE4jGbzcUh_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The second is to define the model itself. \n",
        "# Here, we're going to use Keras preprocessing layers to first convert user ids to integers,\n",
        "# and then convert those to user embeddings via an Embedding layer. \n",
        "# Note that we use the list of unique user ids we computed earlier as a vocabulary\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "kqZYFqYcU0eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The candidate tower"
      ],
      "metadata": {
        "id": "Zg7nifIIVpXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same approach as the query tower\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "3l7DzzRfVq3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}