{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retrieval.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzr9GhCFLkRL3h1dQeNNCe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recommending movies: retrieval\n",
        "\n",
        "Real world Recommender systems are often made up of following steps - \n",
        "\n",
        "* `Retrieval` :   Selecting few thousands of possible candidates to recommend from a set of millions. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
        "\n",
        "* `Ranking` : Drill down thousand possible candidates to few hundreds\n",
        "\n",
        "* `Post-Ranking` : Further refine the candidates to few dozens - Might be helpful in case user is logged from mobile device where screen real estate is limited\n",
        "\n",
        "In this notebook, we will go over Retrieval stage of the Recommender System. Retrieval models are often composed of two sub-models:\n",
        "\n",
        "* A `query model` computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
        "* A `candidate model` computing the candidate representation (an equally-sized vector) using the candidate features\n",
        "\n",
        "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query."
      ],
      "metadata": {
        "id": "V4k3hGcra5I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The Movielens dataset is a classic dataset from the GroupLens research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.\n",
        "\n",
        "The data can be treated in two ways:\n",
        "\n",
        "1. It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of `implicit feedback`, where users' watches tell us which things they prefer to see and which they'd rather not see.\n",
        "\n",
        "2. It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of `explicit feedback`: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.\n",
        "\n",
        "\n",
        "In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example."
      ],
      "metadata": {
        "id": "nO5xXeCdccWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rW2pUo44dDyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-recommenders\n",
        "! pip install -q --upgrade tensorflow-datasets\n",
        "! pip install -q scann\n",
        "! pip install tfds-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1CkxznJc9gF",
        "outputId": "b0e34722-8950-4b64-86fc-00ea7c156000"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.5.2.dev202205140045)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.7.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.64.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.21.6)\n",
            "Requirement already satisfied: etils[epath-no-tf] in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.17.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath-no-tf]->tfds-nightly) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.56.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "1bsMj3gqcxwu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "lkn8USR0dHKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
      ],
      "metadata": {
        "id": "Kkq2B-zqdmh8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at features from ratings datasets\n",
        "for x in ratings.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx7NtrFadprt",
        "outputId": "672a5be0-014c-49ce-c73d-cf6eca3c32ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bucketized_user_age': 45.0,\n",
            " 'movie_genres': array([7]),\n",
            " 'movie_id': b'357',\n",
            " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
            " 'raw_user_age': 46.0,\n",
            " 'timestamp': 879024327,\n",
            " 'user_gender': True,\n",
            " 'user_id': b'138',\n",
            " 'user_occupation_label': 4,\n",
            " 'user_occupation_text': b'doctor',\n",
            " 'user_rating': 4.0,\n",
            " 'user_zip_code': b'53211'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And features from movies datasets\n",
        "for x in movies.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eybWAid3ghdw",
        "outputId": "7d00a80e-2cd9-429c-952a-5c9ede83948c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'movie_genres': array([4]),\n",
            " 'movie_id': b'1681',\n",
            " 'movie_title': b'You So Crazy (1994)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract relevant features for our task - User ID and Movie Title\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "metadata": {
        "id": "WaHwM2qGgndY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split\n",
        "\n",
        "We'll go for 80-20 split"
      ],
      "metadata": {
        "id": "uSiE4aADhRla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "metadata": {
        "id": "SlEBNjjOhffY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also figure out unique user ids and movie titles present in the data.\n",
        "\n",
        "# This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. \n",
        "# To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: \n",
        "# this allows us to look up the corresponding embeddings in our embedding tables."
      ],
      "metadata": {
        "id": "J4zrzoXYhkAE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "unique_movie_titles[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULN-OWCRh73P",
        "outputId": "adacde18-ef08-450c-e82f-311336ee16d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
              "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
              "       b'2 Days in the Valley (1996)',\n",
              "       b'20,000 Leagues Under the Sea (1954)',\n",
              "       b'2001: A Space Odyssey (1968)',\n",
              "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
              "       b'39 Steps, The (1935)'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementation\n",
        "\n",
        "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
      ],
      "metadata": {
        "id": "8q9OLgV-UHZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The query tower"
      ],
      "metadata": {
        "id": "NCnojYPiUKAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first step is to decide on the dimensionality of the query and candidate representations:\n",
        "embedding_dimension = 32\n",
        "\n",
        "# Higher values will correspond to models that may be more accurate, \n",
        "# but will also be slower to fit and more prone to overfitting."
      ],
      "metadata": {
        "id": "NE4jGbzcUh_5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The second is to define the model itself. \n",
        "# Here, we're going to use Keras preprocessing layers to first convert user ids to integers,\n",
        "# and then convert those to user embeddings via an Embedding layer. \n",
        "# Note that we use the list of unique user ids we computed earlier as a vocabulary\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "kqZYFqYcU0eh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The candidate tower"
      ],
      "metadata": {
        "id": "Zg7nifIIVpXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same approach as the query tower\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "3l7DzzRfVq3N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics\n",
        "\n",
        "The input to the model is (user-movie) pair. This will be treated as a positive pair. If our model predicts higher score for the input (user-movie) pair than any other (user-movie) pair, then our models is performing great!\n",
        "\n",
        "To achieve this, we use FactorizedTopK as the metric, which takes as input - candidates.\n",
        "\n",
        "In our case, we will pass movies to compare affinity score wrt the input. These movies need to be mapped to embeddings and we'll the movie model created above"
      ],
      "metadata": {
        "id": "I1jdbXEfcvBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=movies.batch(128).map(movie_model)\n",
        ")"
      ],
      "metadata": {
        "id": "VXnHwPstcx0N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss\n",
        "\n",
        "TFRS has several loss layers and tasks to make this easy.\n",
        "\n",
        "In this instance, we'll make use of the Retrieval task object: a convenience wrapper that bundles together the loss function and metric computation"
      ],
      "metadata": {
        "id": "lzwe9O5AcyZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ],
      "metadata": {
        "id": "I4-SgzI0c0Eo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, \n",
        "and returns the computed loss: we'll use that to implement the model's training loop."
      ],
      "metadata": {
        "id": "nMvHqg_857Rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Model\n",
        "\n",
        "We can now put it all together into a model. TFRS exposes a base model class (tfrs.models.Model) which streamlines building models: all we need to do is to set up the components in the `__init__` method, and implement the compute_loss method, taking in the raw features and returning a loss value.\n",
        "\n",
        "The base model will then take care of creating the appropriate training loop to fit our model."
      ],
      "metadata": {
        "id": "JGIGIASR6D9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model,\n",
        "    # getting embeddings back.\n",
        "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(user_embeddings, positive_movie_embeddings)"
      ],
      "metadata": {
        "id": "sTgrJqoY6kUY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tfrs.Model base class is a simply convenience class: \n",
        "\n",
        "it allows us to compute both training and test losses using the same method."
      ],
      "metadata": {
        "id": "7OHaPD577AgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting and evaluating\n",
        "\n",
        "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
        "\n",
        "Let's first instantiate the model."
      ],
      "metadata": {
        "id": "YcxY8PtZ7F4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MovielensModel(user_model, movie_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "metadata": {
        "id": "VOIJbYRq7Jx9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then shuffle, batch, and cache the training and evaluation data.\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "metadata": {
        "id": "qj4ycRNf7QW3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(cached_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-_DPjir7WAl",
        "outputId": "a662e71a-b769-4267-be8d-7936b6bba53b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 29s 2s/step - factorized_top_k/top_1_categorical_accuracy: 2.8750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0042 - factorized_top_k/top_10_categorical_accuracy: 0.0111 - factorized_top_k/top_50_categorical_accuracy: 0.0794 - factorized_top_k/top_100_categorical_accuracy: 0.1548 - loss: 69885.1129 - regularization_loss: 0.0000e+00 - total_loss: 69885.1129\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0021 - factorized_top_k/top_5_categorical_accuracy: 0.0170 - factorized_top_k/top_10_categorical_accuracy: 0.0346 - factorized_top_k/top_50_categorical_accuracy: 0.1627 - factorized_top_k/top_100_categorical_accuracy: 0.2866 - loss: 67523.3672 - regularization_loss: 0.0000e+00 - total_loss: 67523.3672\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 27s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0029 - factorized_top_k/top_5_categorical_accuracy: 0.0212 - factorized_top_k/top_10_categorical_accuracy: 0.0442 - factorized_top_k/top_50_categorical_accuracy: 0.1857 - factorized_top_k/top_100_categorical_accuracy: 0.3137 - loss: 66302.9574 - regularization_loss: 0.0000e+00 - total_loss: 66302.9574\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 25s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0249 - factorized_top_k/top_10_categorical_accuracy: 0.0495 - factorized_top_k/top_50_categorical_accuracy: 0.2007 - factorized_top_k/top_100_categorical_accuracy: 0.3321 - loss: 65609.7436 - regularization_loss: 0.0000e+00 - total_loss: 65609.7436\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 25s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0034 - factorized_top_k/top_5_categorical_accuracy: 0.0274 - factorized_top_k/top_10_categorical_accuracy: 0.0543 - factorized_top_k/top_50_categorical_accuracy: 0.2131 - factorized_top_k/top_100_categorical_accuracy: 0.3464 - loss: 65110.2692 - regularization_loss: 0.0000e+00 - total_loss: 65110.2692\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0291 - factorized_top_k/top_10_categorical_accuracy: 0.0572 - factorized_top_k/top_50_categorical_accuracy: 0.2230 - factorized_top_k/top_100_categorical_accuracy: 0.3587 - loss: 64721.4297 - regularization_loss: 0.0000e+00 - total_loss: 64721.4297\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0306 - factorized_top_k/top_10_categorical_accuracy: 0.0604 - factorized_top_k/top_50_categorical_accuracy: 0.2310 - factorized_top_k/top_100_categorical_accuracy: 0.3689 - loss: 64408.9602 - regularization_loss: 0.0000e+00 - total_loss: 64408.9602\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 28s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0040 - factorized_top_k/top_5_categorical_accuracy: 0.0315 - factorized_top_k/top_10_categorical_accuracy: 0.0627 - factorized_top_k/top_50_categorical_accuracy: 0.2364 - factorized_top_k/top_100_categorical_accuracy: 0.3758 - loss: 64153.3757 - regularization_loss: 0.0000e+00 - total_loss: 64153.3757\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 25s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0037 - factorized_top_k/top_5_categorical_accuracy: 0.0323 - factorized_top_k/top_10_categorical_accuracy: 0.0646 - factorized_top_k/top_50_categorical_accuracy: 0.2416 - factorized_top_k/top_100_categorical_accuracy: 0.3817 - loss: 63941.4311 - regularization_loss: 0.0000e+00 - total_loss: 63941.4311\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0336 - factorized_top_k/top_10_categorical_accuracy: 0.0662 - factorized_top_k/top_50_categorical_accuracy: 0.2443 - factorized_top_k/top_100_categorical_accuracy: 0.3866 - loss: 63763.4155 - regularization_loss: 0.0000e+00 - total_loss: 63763.4155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f940b072510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`IMPORTANT`\n",
        "\n",
        "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. \n",
        "These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. \n",
        "\n",
        "> For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time.\n",
        "\n",
        "Note that, in this example, we evaluate the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be prudent to turn metric calculation off in training, and only run it in evaluation."
      ],
      "metadata": {
        "id": "1H8sikbN7Zwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can evaluate our model on the test set:\n",
        "\n",
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aiYWbMR8c4v",
        "outputId": "c1b9c9aa-7561-4d88-f4ef-f7c328f2a5eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 8s 984ms/step - factorized_top_k/top_1_categorical_accuracy: 2.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0033 - factorized_top_k/top_10_categorical_accuracy: 0.0100 - factorized_top_k/top_50_categorical_accuracy: 0.0839 - factorized_top_k/top_100_categorical_accuracy: 0.1819 - loss: 31678.6315 - regularization_loss: 0.0000e+00 - total_loss: 31678.6315\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_100_categorical_accuracy': 0.18185000121593475,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.010049999691545963,\n",
              " 'factorized_top_k/top_1_categorical_accuracy': 0.0002500000118743628,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.08389999717473984,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.0032999999821186066,\n",
              " 'loss': 28730.44140625,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 28730.44140625}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test set performance is much worse than training performance. This is due to two factors:\n",
        "\n",
        "Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
        "The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
        "The second phenomenon can be tackled by excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature, but we don't follow it in these tutorials. If not recommending past watches is important, we should expect appropriately specified models to learn this behaviour automatically from past user history and contextual information. Additionally, it is often appropriate to recommend the same item multiple times (say, an evergreen TV series or a regularly purchased item)."
      ],
      "metadata": {
        "id": "23ZNGjsF8khd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions\n",
        "\n",
        "Now that we have a model, we would like to be able to make predictions. We can use 2 ways for predictions - \n",
        "\n",
        "1. Brute Force way -  We can use the tfrs.layers.factorized_top_k.BruteForce layer to do this. The brute force layers retreives the output using an exhaustive search. Kinda slow method, but works!\n",
        "\n",
        "2. Approximate retrieval index using ScanN - We can also export an approximate retrieval index to speed up predictions. This will make it possible to efficiently surface recommendations from sets of tens of millions of candidates.This layer will perform approximate lookups: this makes retrieval slightly less accurate, but orders of magnitude faster on large candidate sets."
      ],
      "metadata": {
        "id": "zd8rMykK8oBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brute Force Method"
      ],
      "metadata": {
        "id": "Mu5NcdAa-JNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "# recommends movies out of the entire movies dataset.\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
        ")\n",
        "\n",
        "# Get recommendations.\n",
        "_, titles = index(tf.constant([\"42\"]))\n",
        "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvHxO66F9L0q",
        "outputId": "23ae3256-5b24-48b2-87c7-beb844bc0813"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations for user 42: [b'Bridges of Madison County, The (1995)' b'Rudy (1993)'\n",
            " b'Aristocats, The (1970)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approximate Lookup using ScanN\n",
        "\n",
        "we can use the scann package. This is an optional dependency of TFRS, and we installed it separately at the beginning of this tutorial by calling !pip install -q scann."
      ],
      "metadata": {
        "id": "cufUczFy-Mt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
        "scann_index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
        ")\n",
        "\n",
        "# Get recommendations.\n",
        "_, titles = scann_index(tf.constant([\"42\"]))\n",
        "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bov9qNHg9rkF",
        "outputId": "d5789dee-cb8e-49c9-b070-1154c04b33d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations for user 42: [b'Bridges of Madison County, The (1995)' b'Affair to Remember, An (1957)'\n",
            " b'Rent-a-Kid (1995)']\n"
          ]
        }
      ]
    }
  ]
}